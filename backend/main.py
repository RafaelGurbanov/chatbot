import os
import re
import json
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Any, Dict, List, Tuple

from rapidfuzz import fuzz

APP_DIR = os.path.dirname(__file__)

DATA_PATH_1 = os.path.join(os.path.dirname(APP_DIR), "data", "problems.json")
DATA_PATH_2 = os.path.join(APP_DIR, "data", "problems.json")
DATA_PATH = DATA_PATH_1 if os.path.exists(DATA_PATH_1) else DATA_PATH_2

app = FastAPI(title="Autism Chatbot API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None


# ===== Smart JSON cache =====
KB: List[Dict[str, Any]] = []
UI: Dict[str, Any] = {}
MTIME: float = 0.0


def norm_list(x: Any) -> List[str]:
    if not x:
        return []
    if isinstance(x, str):
        return [x.strip().lower()]
    if isinstance(x, list):
        out = []
        for v in x:
            if v is None:
                continue
            s = str(v).strip().lower()
            if s:
                out.append(s)
        return out
    s = str(x).strip().lower()
    return [s] if s else []


# ====== Universal keyword bank (bÃ¼tÃ¼n mÃ¶vzular Ã¼Ã§Ã¼n) ======
KEYBANK: Dict[str, List[str]] = {
    "auditory": [
        "qulaq", "qulaÄŸ", "qulaqlar", "sÉ™s", "ses", "sÉ™s-kÃ¼y", "ses-kuy",
        "tozsoran", "fen", "ventilyasiya", "ventilyator", "maÅŸÄ±n sÉ™si", "masin sesi",
        "qÄ±ÅŸqÄ±r", "bagir", "sirena", "metro", "toy"
    ],
    "smell": ["qoxu", "iy", "É™tir", "parfum", "tÉ™mizlik", "xlor", "yemÉ™k qoxusu"],
    "touch": ["toxun", "paltar", "etiket", "corab", "corab", "dÃ¼ymÉ™", "daraq", "saÃ§"],
    "sleep": ["yuxu", "yatmÄ±r", "oyanÄ±r", "gecÉ™", "rejim"],
    "toilet": ["tualet", "pampers", "bez", "unitaz", "karÅŸok", "sidik", "nÉ™cis"],
    "food": ["yemÉ™k", "qida", "dad", "tekstura", "seÃ§ir", "yemir", "qlÃ¼ten", "kazein"],
    "meltdown": ["meltdown", "isterika", "qÄ±ÅŸqÄ±rÄ±r", "baÄŸÄ±rÄ±r", "aqressiya", "vurur", "diÅŸlÉ™yir"],
    "hygiene": ["diÅŸ", "fÄ±rÃ§a", "pasta", "duÅŸ", "Ã§immÉ™k", "dÄ±rnaq"],
    "school": ["mÉ™ktÉ™b", "baÄŸÃ§a", "mÃ¼É™llim", "dÉ™rs", "inklyuziv"],
    "communication": ["danÄ±ÅŸmÄ±r", "Ã¼nsiyyÉ™t", "jest", "pecs", "exolaliya", "tÉ™krar edir"],
}


def split_into_chunks(text: str) -> List[str]:
    """
    Uzun mÉ™tni 'baÅŸlÄ±qlar' / boÅŸ sÉ™tirlÉ™r / bullet-lÉ™rÉ™ gÃ¶rÉ™ hissÉ™lÉ™rÉ™ bÃ¶lÃ¼r.
    MÉ™tnin Ã¶zÃ¼nÃ¼ dÉ™yiÅŸmirik, sadÉ™cÉ™ hissÉ™lÉ™rÉ™ ayÄ±rÄ±rÄ±q.
    """
    if not text:
        return []

    # Normalizasiya
    t = text.replace("\r\n", "\n").strip()

    # 1) BaÅŸlÄ±qlara gÃ¶rÉ™ bÃ¶l ( **...** vÉ™ ya sÉ™tirdÉ™ ':' olanlar )
    # 2) Sonra bÃ¶yÃ¼k boÅŸluqlara gÃ¶rÉ™ bÃ¶l
    parts = re.split(r"\n{2,}", t)
    chunks = []
    for p in parts:
        p = p.strip()
        if len(p) < 80:
            continue
        chunks.append(p)
    return chunks if chunks else [t]


def extract_keywords(title: str, text: str) -> List[str]:
    """
    Avtomatik keyword Ã§Ä±xarÄ±r:
    - title sÃ¶zlÉ™ri
    - KEYBANK-lÉ™ mÃ¶vzu iÅŸarÉ™lÉ™ri
    """
    title_l = (title or "").lower()
    text_l = (text or "").lower()

    kws = set()

    # title-dan sÃ¶zlÉ™r
    for w in re.findall(r"[a-zÉ™ÄŸÄ±Ã¶ÅŸÃ¼Ã§0-9]{3,}", title_l):
        kws.add(w)

    # keybank match
    for _, vocab in KEYBANK.items():
        for v in vocab:
            if v in title_l or v in text_l:
                kws.add(v)

    # praktik â€œifadÉ™lÉ™râ€
    if "qulaq" in text_l and ("aÄŸlay" in text_l or "aglay" in text_l):
        kws.add("qulaqlarÄ±nÄ± tutub aÄŸlayÄ±r")
        kws.add("qulaqlarini tutub aglayir")

    return sorted(kws)


def build_query_text(item: Dict[str, Any]) -> str:
    title = str(item.get("title", "")).lower()
    topic = str(item.get("topic", "")).lower()
    keywords = " ".join(norm_list(item.get("keywords")))
    text = str(item.get("text", "")).lower()   # ğŸ”¥ ÆN VACÄ°B

    # Ã§ox uzun olmasÄ±n deyÉ™ ilk 800 simvol kifayÉ™tdir
    text = text[:800]

    return f"{title} {topic} {keywords} {text}".strip()


def keyword_overlap(user_text: str, keywords: List[str]) -> int:
    c = 0
    for kw in keywords:
        if kw and kw in user_text:
            c += 1
    return c


def top2_match(user_text: str) -> Tuple[Tuple[float, Optional[Dict[str, Any]]], Tuple[float, Optional[Dict[str, Any]]]]:
    scored: List[Tuple[float, Dict[str, Any], int]] = []

    for item in KB:
        kws = norm_list(item.get("keywords"))
        overlap = keyword_overlap(user_text, kws)

        q = build_query_text(item)
        if not q:
            continue

        s1 = fuzz.token_set_ratio(user_text, q)
        s2 = fuzz.partial_ratio(user_text, q)
        score = float(max(s1, s2))   # âœ… score burda yaranÄ±r

        # overlap bonus
        score += min(overlap * 4.0, 16.0)

        # overlap==0 Ã¼Ã§Ã¼n cÉ™za (amma score hesablandÄ±qdan sonra!)
        if overlap == 0:
            score -= 5.0

        scored.append((score, item, overlap))

    scored.sort(key=lambda x: x[0], reverse=True)

    top1 = (scored[0][0], scored[0][1]) if len(scored) > 0 else (0.0, None)
    top2 = (scored[1][0], scored[1][1]) if len(scored) > 1 else (0.0, None)
    return top1, top2

def load_json() -> None:
    global KB, UI, MTIME

    try:
        mtime = os.path.getmtime(DATA_PATH)
    except FileNotFoundError:
        KB = []
        UI = {}
        MTIME = 0.0
        return

    if mtime == MTIME:
        return

    with open(DATA_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    UI = data.get("ui", {}) or {}

    kb: List[Dict[str, Any]] = []

    # 1) items schema (qÄ±sa cavablar)
    for it in (data.get("items") or []):
        kb.append({
            "id": it.get("id"),
            "title": it.get("title", ""),
            "topic": it.get("topic", ""),
            "keywords": norm_list(it.get("keywords")),
            "text": it.get("text", ""),
            "source": "items",
        })

    # 2) problems schema (ekspert bÃ¶yÃ¼k mÉ™tnlÉ™r) â†’ chunk-lara bÃ¶l
    for p in (data.get("problems") or []):
        pid = p.get("id")
        title = p.get("title", "")
        desc = p.get("description", "")
        image = p.get("image")

        chunks = split_into_chunks(desc)
        for idx, ch in enumerate(chunks):
            kws = extract_keywords(title, ch)

            kb.append({
                "id": f"problem_{pid}_chunk_{idx}",
                "parent_id": pid,
                "title": title,
                "topic": "expert_article",
                "keywords": kws,
                "text": ch,              # âœ… mÃ¼tÉ™xÉ™ssis mÉ™tni eyni qalÄ±r (hissÉ™-hissÉ™)
                "source": "problems",
                "image": image,
            })

    KB = kb
    MTIME = mtime


@app.get("/config")
def config():
    load_json()
    return {
        "botName": UI.get("botName", "Autism Support Chatbot"),
        "subtitle": UI.get("subtitle", "RÉ™smi dÉ™stÉ™k â€¢ Praktik cavablar"),
        "welcomeMessage": UI.get(
            "welcomeMessage",
            "Salam. SuallarÄ±nÄ±zÄ± yaza bilÉ™rsiniz â€” mÉ™n addÄ±m-addÄ±m cavablandÄ±racaÄŸam."
        ),
        "inputPlaceholder": UI.get("inputPlaceholder", "MesajÄ±nÄ±zÄ± yazÄ±n..."),
        "buttonText": UI.get("buttonText", "Chat"),
    }


@app.get("/health")
def health():
    load_json()
    return {"ok": True, "data_path": DATA_PATH, "kb_items_count": len(KB)}


@app.post("/chat")
def chat(req: ChatRequest):
    load_json()
    user_text = (req.message or "").strip().lower()

    if not user_text:
        return {"answer": "ZÉ™hmÉ™t olmasa sualÄ±nÄ±zÄ± yazÄ±n.", "used_context": False, "sources": []}

    (s1, item1), (s2, _) = top2_match(user_text)

    # âœ… universal qÉ™rar qaydasÄ± (bÃ¼tÃ¼n suallar Ã¼Ã§Ã¼n)
    THRESHOLD = 80.0  # yÃ¼ksÉ™k â†’ lazÄ±msÄ±z cavablarÄ± kÉ™sir
    MARGIN = 4.0

    if item1 and s1 >= THRESHOLD and (s1 - s2) >= MARGIN:
        return {
            "answer": item1.get("text", "Cavab mÃ¶vcud deyil."),
            "used_context": True,
            "sources": [{
                "id": item1.get("id"),
                "title": item1.get("title"),
                "source": item1.get("source"),
                "score": round(s1, 2),
                "parent_id": item1.get("parent_id"),
            }]
        }

    # Æmin deyilsÉ™: lazÄ±msÄ±z cavab yoxdur, yalnÄ±z dÉ™qiqlÉ™ÅŸdirmÉ™
    return {
        "answer": (
            "SualÄ±nÄ±zÄ± dÉ™qiq tutmadÄ±m. ZÉ™hmÉ™t olmasa 1-2 detal É™lavÉ™ edin:\n"
            "â€¢ YaÅŸ neÃ§É™dir?\n"
            "â€¢ NÉ™ tetiklÉ™yir? (sÉ™s, iÅŸÄ±q, qoxu, toxunuÅŸ, rutin pozulmasÄ±)\n"
            "â€¢ NÉ™ qÉ™dÉ™r tez-tez olur?\n"
            "MÉ™s: â€œ5 yaÅŸ, tozsoran sÉ™sindÉ™ qulaqlarÄ±nÄ± tutub aÄŸlayÄ±râ€."
        ),
        "used_context": False,
        "sources": []
    }
